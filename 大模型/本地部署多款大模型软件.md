---
created: 2024-10-09T10:41:08 (UTC +08:00)
tags: []
source: https://mp.weixin.qq.com/s/qd-lzOgAoyHR1E2IgCzYPQ
author: 
---

# Github 88.8k Stars 一款可以本地部署多款大模型软件

> ## Excerpt
> 只需* 戳上方蓝字“牛皮糖不吹牛”关注我    大家好，我是牛皮糖！

---
只需\* 戳上方蓝字“**牛皮糖不吹牛**”关注我

    大家好，我是牛皮糖！这个之前其实有介绍过，但是没啥人看到，我觉得这个还是对于想学习大模型的同学来说还是挺好的。Ollama 是一个开源框架，专为在本地机器上便捷部署和运行大型语言模型（LLM）而设计。

![图片](https://mmbiz.qpic.cn/mmbiz_png/p1ESIQQvfrT08J7B7ysl89kFU2jHdJBmFjjBohs1FiaDMDFy4zibBeoThqeQyiaDLvZe9aV6UGrrCI7RbDuf1vc8A/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

**支持大模型**

    Ollama 本身就有多款大模型可以下载，但是要根据自己的电脑下载对应配置的参数。  

预构建模型库：包含一系列预先训练好的大型语言模型，用户可以直接选用这些模型应用于自己的应用程序，无需从头训练或自行寻找模型源。

___

您应该至少有 8 GB 的 RAM 来运行 7B 型号，16 GB 的 RAM 来运行 13B 型号，32 GB 的 RAM 来运行 33B 型号。

| 大模型 | 参数 | 大小 | 命令 |
| --- | --- | --- | --- |
| Llama 3.1 | 8B | 4.7GB | `ollama run llama3.1` |
| Llama 3.1 | 70B | 40GB | `ollama run llama3.1:70b` |
| Llama 3.1 | 405B | 231GB | `ollama run llama3.1:405b` |
| Phi 3 Mini | 3.8B | 2.3GB | `ollama run phi3` |
| Phi 3 Medium | 14B | 7.9GB | `ollama run phi3:medium` |
| Gemma 2 | 2B | 1.6GB | `ollama run gemma2:2b` |
| Gemma 2 | 9B | 5.5GB | `ollama run gemma2` |
| Gemma 2 | 27B | 16GB | `ollama run gemma2:27b` |
| Mistral | 7B | 4.1GB | `ollama run mistral` |
| Moondream 2 | 1.4B | 829MB | `ollama run moondream` |
| Neural Chat | 7B | 4.1GB | `ollama run neural-chat` |
| Starling | 7B | 4.1GB | `ollama run starling-lm` |
| Code Llama | 7B | 3.8GB | `ollama run codellama` |
| Llama 2 Uncensored | 7B | 3.8GB | `ollama run llama2-uncensored` |
| LLaVA | 7B | 4.5GB | `ollama run llava` |
| Solar | 10.7B | 6.1GB | `ollama run solar` |

简化部署：Ollama 目标在于简化在 Docker 容器中部署大型语言模型的过程，使得非专业用户也能方便地管理和运行这些复杂的模型。

![图片](https://mmbiz.qpic.cn/mmbiz_png/p1ESIQQvfrT08J7B7ysl89kFU2jHdJBmIN1fJ10uqFibwtVOapqX5m0apMChPvksaV8aW0czhvwtJibWZUKhZlBA/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

轻量级与可扩展：作为轻量级框架，Ollama 保持了较小的资源占用，同时具备良好的可扩展性，允许用户根据需要调整配置以适应不同规模的项目和硬件条件。

API支持：提供了一个简洁的 API，使得开发者能够轻松创建、运行和管理大型语言模型实例，降低了与模型交互的技术门槛。

模型导入与定制：  

Ollama 也支持 Modelfile 中导入 GGUF 模型，只需要创建一个名为 的文件Modelfile，其中包含FROM要导入的模型的本地文件路径的指令。然后再再Ollama 中创建一个模型。

**配合页面**

  还能配合FastGPT 就可以整合本地资源对大模型上传文件了,等Open-UI 等做 本地的大模型部署。

![图片](https://mmbiz.qpic.cn/mmbiz_png/p1ESIQQvfrT08J7B7ysl89kFU2jHdJBmRgXwdw45DQqBwXbWsTpo7IX6tLicnnicdibJGZRQ04xiaXE1xETvqb6Slw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1&wx_co=1)

___

  这个很吃电脑配置。

感兴趣的还是可以去GitHub 进行尝试或者看看有什么好的注意。

项目地址：

**https://github.com/ollama/ollama**

![图片](data:image/svg+xml,%3C%3Fxml version='1.0' encoding='UTF-8'%3F%3E%3Csvg width='1px' height='1px' viewBox='0 0 1 1' version='1.1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink'%3E%3Ctitle%3E%3C/title%3E%3Cg stroke='none' stroke-width='1' fill='none' fill-rule='evenodd' fill-opacity='0'%3E%3Cg transform='translate(-249.000000, -126.000000)' fill='%23FFFFFF'%3E%3Crect x='249' y='126' width='1' height='1'%3E%3C/rect%3E%3C/g%3E%3C/g%3E%3C/svg%3E)

·················END·················

### **推荐阅读**

•   [github 95.5k Star 的项目集合地](http://mp.weixin.qq.com/s?__biz=MzkyNDYyODg0MQ==&mid=2247485137&idx=1&sn=00da6656ecacfcab683c6e149c208985&chksm=c1d3a4d7f6a42dc107aa9233b7a23f20a4bfe7bcd9d48a0fe24787faae37539bd65663d8ba8e&scene=21#wechat_redirect)[](http://mp.weixin.qq.com/s?__biz=MzkyNDYyODg0MQ==&mid=2247485121&idx=1&sn=97093dfe7da78fb786bb999a284ee1fc&chksm=c1d3a4c7f6a42dd1df4cb4de4c057671d57274480eac57e61b4f6bae86aef03ff26bf23ffdd6&scene=21#wechat_redirect)•   [4核 16G 就能 RAGFlow Quick start 快速入门](http://mp.weixin.qq.com/s?__biz=MzkyNDYyODg0MQ==&mid=2247485121&idx=1&sn=97093dfe7da78fb786bb999a284ee1fc&chksm=c1d3a4c7f6a42dd1df4cb4de4c057671d57274480eac57e61b4f6bae86aef03ff26bf23ffdd6&scene=21#wechat_redirect)•   [github 7.8k star 将小爱音箱接入 ChatGPT 和豆包，改造成你的专属语音助手。](http://mp.weixin.qq.com/s?__biz=MzIxODg1OTk1MA==&mid=2247488494&idx=1&sn=0244c0a45012f5a6ca6cdf9a0ac88024&chksm=97e5432fa092ca39991868bda959cfd8bafd949fa445ef72bdca5cc6c4dc2ed4bd51e22634f8&scene=21#wechat_redirect)

___
